var tipuesearch = {"pages":[{"title":"About","text":"Je suis John Boby Mesadieu , data scientist à SOLUTIONS SA . Mes interêts incluent la statistique, l'analyse de données, le machine learning, la programmation en Python, le développement mobile IOS et Android. Ce blog est destiné à tous ceux qui souhaitent travailler ou qui travaillent actuellement avec des données, en particulier ceux qui utilisent Python pour y parvenir. L'idée de ce projet est de donner un aperçu des méthodes les plus pertinentes pour pratiquer la data science, couvrant des domaines tels que la statistique, le machine learning et la visualisation de données.","tags":"About","url":"https://JBobyM.github.io/pages/about.html","loc":"https://JBobyM.github.io/pages/about.html"},{"title":"Prédire musique hit en utilisant Spotify","text":"Introduction Des milliers de chansons voient le jour tous les ans dans le monde. Certaines connaissent de vrais succès dans l'industrie musicale ; d'autres le sont moins. Il est un fait que réussir dans cette industrie demeure une tâche difficile. Investir dans la production d'une chanson requiert des activités diversifiées et peuvent consommer beaucoup de ressources. Il n'y a aucun outil jusque-là automatique dont les artistes et producteurs pourraient s'en servir pour évaluer si leur chanson qu'ils sont sur le point de publier va être un ‘Hit'. Dans ce projet, on cherchera à comprendre ce qui caractérise une chanson populaire, et plus précisement si l'on pourrait prédire la popularité d'une chanson seulement en nous basant sur ses caractéristiques d'audio et celles de l'artiste. Nous allons construire un modèle-classeur de machine learning pouvant classer une chanson en Hit ou non-Hit. Bien que des facteurs sociaux comme le contexte dans lequel la chanson a été diffusée, la demographie de ses auditeurs et l'effectivite de sa campagne de marketing peuvent tout aussi bien jouer un role important dans sa viralité, nous emettons l'hypothese que les caractéristiques inhérentes à une chanson, tels que son nom, sa duréee, ses caractéristiques d'audio et la popularité de l'artiste peuvent être correlés et également révélateur de sa viralité. Construction du jeu de données // Chagement de temps des verbes a continuer Dans cet article de deux parties, nous allons essayer de créer un modèle de machine learning comme un classeur de chansons en Hit ou non-Hit. Pour réussir un tel travail, nous aurons besoin de plus de données que possible. Etant donné qu'on n'a pas pu trouver un dataset venant d'une seule et unique source avec toutes les variables, il nous a fallu recourir au data enrichment à mesure que cela ait été nécessaire. Nous avons eu besoin de trois sources pour construire notre dataset. Et l'opération de recueillement de données s'est déroulée de la maniere suivante : Billboard : On s'est servi de la page Wikipédia de Bilboard Year-End Hot 100 pour recueillir les musiques les plus populaires pour les années allant de 2010 à 2019. On utilisera le web scrapping pour accomplir cette tâche. Spotify : Ensuite on utilisera la librairie Spotipy pour récupérer les caractéristiques relatives à l'audio tels que la dansabilité, la vivacité, l'instrumentalité, etc. et à l'artiste comme la popularité, le nombre de followers etc. à la fois des chansons hits provenant du Billboard Year-End Hot 100 et d'autres chansons non hits sur la période. LyricsOnDemand : Enfin LyricsOnDemand sera utilisé principalement pour récupérer les paroles des chansons. On considèrera qu'une musique de notre dataset est Hit si elle a fait partie du classement du Billboard Year-End Hot 100 au moins une fois pendant l'une des années sur la période considérée. Nous avons executé pas mal de lignes de codes afin de construire notre jeu de données. Tous les codes du projet sont accessibles depuis ma page github . Outils: La librairie Spotipy pour acceder aux données de la plateforme musicale de Spotify Seaborn et matplotlib pour la visualisation des données Pandas et numpy pour l'analyse des données Sklearn pour la construction du modele de machine learning Les variables Spotify est l'une des plus grandes plateformes de streaming dans le monde. A l'instar de Twitter ou Facebook, elle fournit une API (application programming interface) pour que les developpeurs puissent interagir avec son immense base de données musicale. Via cet API, j'ai pu recolleter des données pour plus de 22 000 chansons; chaque chanson étant caracterisée par à peu pres une vingtaine de variables. Les variables retournées par l'API sont aussi riches en information que variées. Toutefois, j'ai selectionné uniquement celles qui sont jugées pertinentes pour le travail et pratiqué du feature engeneering sur elles afin de préparer le jeu de données au mieux pour l'entrainerment du modele. Vous trouverez ci-dessous la description de chaque variable utilisée. Les variables relatives à l'artiste nom_artiste : Nom de l'artiste principal de la chanson. Popularité : La popularité de l'artiste sur Spotify. La valeur sera comprise entre 0 et 100, 100 étant le plus populaire. Nous pensons que plus un artiste est populaire, plus une chanson sur laquelle il pose sa voix est susceptible d'être virale. Followers : Nombre total de followers sur Spotify. Les variables relatives à la chanson Date de sortie : Date à laquelle la chanson a été diffusée pour la première fois. Marché disponible : Nombre de pays dans lequel la chanson peut être jouée. Durée : La durée de la chanson en millisecondes. Acoustique : Une mesure de confiance de 0.0 à 1.0 indiquant si la chanson est acoustique. 1.0 représente une confiance élevée que la chanson est acoustique. Dansabilité : La dansabilité décrit à quel point une chanson est adaptée à la danse sur la base d'une combinaison d'éléments musicaux, notamment le tempo, la stabilité du rythme, la force du rythme et la régularité globale. Une valeur de 0.0 est la moins dansante et 1.0 est la plus dansante. Energie : L'énergie est une mesure de 0.0 à 1.0 et représente une mesure perceptuelle de l'intensité et de l'activité. En règle générale, les chansons énergiques sont rapides, sonores et bruyantes. Instrumentalité : Prédit si une chanson ne contient pas de voix. Les sons «Ooh» et «aah» sont traités comme instrumentaux dans ce contexte. Les morceaux de rap ou de mots parlés sont clairement «vocaux». Plus la valeur instrumentale est proche de 1.0, plus la piste ne contient aucun contenu vocal. Les valeurs supérieures à 0.5 sont censées représenter des chansons instrumentales, mais la confiance est plus élevée lorsque la valeur approche de 1.0. Vivacité : Détecte la présence d'un public dans l'enregistrement. Des valeurs de vivacité plus élevées représentent une probabilité accrue que la chanson ait été jouée en direct. Une valeur supérieure à 0.8 offre une forte probabilité que la chanson soit en direct. Intensité : L'intensité global d'une chanson en décibels (dB). Les valeurs d'intensité sont moyennées sur toute la chanson et sont utiles pour comparer l'intensité relative des chansons. L'intensité sonore est la qualité d'un son qui est le principal corrélat psychologique de la force physique (amplitude). Les valeurs varient entre -60 et 0 db. Eloquence : L'éloqunece détecte la présence de mots prononcés dans une chanson. Plus l'enregistrement est exclusivement vocal (par exemple, talk-show, livre audio, poésie), plus la valeur d'attribut est proche de 1.0. Les valeurs supérieures à 0.66 décrivent des chansons qui sont probablement entièrement constituées de mots prononcés. Les valeurs comprises entre 0.33 et 0.66 décrivent des pistes qui peuvent contenir à la fois de la musique et de la parole, soit en sections, soit en couches, y compris des cas comme la musique rap. Les valeurs inférieures à 0.33 représentent très probablement de la musique et d'autres pistes non vocales. Valence : Une mesure de 0.0 à 1.0 décrivant la positivité musicale véhiculée par une chanson. Les chansons avec une valence élevée semblent plus positives (par exemple, joyeuses, gaies, euphoriques), tandis que les chansons avec une valence basse semblent plus négatives (par exemple tristes, déprimées, en colère). Tempo : Le tempo global estimé d'une chanson en battements par minute (BPM). Dans la terminologie musicale, le tempo est la vitesse ou le rythme d'une pièce donnée et découle directement de la durée moyenne des temps. hit : Variable dichotomique mesurant si une chanson est hit ou non. Elle prend la valeur de 1 si la chanson est hit, 0 sinon. C'est notre variable dépendante, c'est-à-dire celle que l'on cherchera à prédire pour une chanson donnée. featuring : Variable dépendante dichotomique mesurant s'il y a un ou plusieurs artistes invités sur une chanson. mois : Mois de sortie de la chanson. jour_sem : Jour de la semaine durant lequel la chanson a ete diffusee. jour : Jour de sortie de la chanson. Certaines de ces variables sont utilisées uniquement pour l'analyse, d'autres participent à toutes les étapes du pipeline. A présent, jettons un coup d'oeil sur la dimension de notre jeu de données. In [21]: # Cellule à enlever import pandas as pd import numpy as np df1 = pd . read_csv ( r 'C:\\Users\\Kathee\\Documents\\Blog content\\bilboard_data.csv' ) df1 . drop ([ 'key' ], axis = 1 , inplace = True ) df2 = pd . read_csv ( r 'C:\\Users\\Kathee\\Documents\\Blog content\\spotify_data_20000.csv' ) df1 [ 'pop' ] = 1 df2 [ 'pop' ] = 0 df = pd . concat ([ df1 , df2 ]) def feat_ ( row ): if 'feat.' in row [ 'track_name' ]: val = 1 else : val = 0 return val df [ 'featuring' ] = df . apply ( feat_ , axis = 1 ) df [ 'search' ] = df [ 'artist_name' ] + \" \" + df [ \"track_name\" ] df = df . drop_duplicates ( subset = [ 'search' ], keep = 'first' ) df . shape df = df . rename ( columns = { 'artist_name' : 'nom_artiste' , 'pop_artist' : 'pop_artiste' , 'track_name' : 'nom_chanson' , 'avail_mark' : 'marche_disp' , 'rel_date' : 'date_sortie' , 'pop_track' : 'pop_chanson' , 'acousticness' : 'acoustique' , 'danceability' : 'dansabilite' , 'duration_ms' : 'duree' , 'energy' : 'energie' , 'instrumentalness' : 'instrumentalite' , 'liveness' : 'vivacite' , 'loudness' : 'intensite' , 'speechiness' : 'eloquence' , 'tempo' : 'tempo' , 'valence' : 'valence' , 'pop' : 'hit' , 'featuring' : 'featuring' }) df [ 'date_sortie' ] = pd . to_datetime ( df [ 'date_sortie' ]) df [ 'mois_sortie' ] = df [ 'date_sortie' ] . apply ( lambda m : m . month ) df [ 'jour_sortie' ] = df [ 'date_sortie' ] . apply ( lambda d : d . day ) df [ 'jour_sem_sortie' ] = df [ 'date_sortie' ] . apply ( lambda w : w . weekday ()) In [22]: df . shape Out[22]: (19182, 24) Après avoir choisi les variables disponibles jugées pertinentes pour le modèle, supprimé les observations contenant des valeurs manquantes et dupliquées, notre jeu se reduit à 19 120 observations contenat 24 variables chacune. In [23]: df . head ( n = 5 ) Out[23]: nom_artiste pop_artiste tot_followers nom_chanson marche_disp date_sortie pop_chanson acoustique dansabilite duree ... eloquence tempo time_signature valence hit featuring search mois_sortie jour_sortie jour_sem_sortie 0 Kesha 81 5470072 TiK ToK 79 2010-01-01 79 0.09910 0.755 199693 ... 0.1420 120.028 4 0.714 1 0 Kesha TiK ToK 1 1 4 1 Lady Antebellum 74 2933375 Need You Now 78 2010-01-01 69 0.09270 0.587 277573 ... 0.0303 107.943 4 0.231 1 0 Lady Antebellum Need You Now 1 1 4 2 Train 77 3249275 Hey, Soul Sister 79 2010-12-01 82 0.18500 0.673 216773 ... 0.0431 97.012 4 0.795 1 0 Train Hey, Soul Sister 12 1 2 3 Katy Perry 87 15347727 California Gurls 79 2012-03-12 72 0.00446 0.791 234653 ... 0.0569 125.014 4 0.425 1 0 Katy Perry California Gurls 3 12 0 4 Usher 82 7654666 OMG (feat. will.i.am) 79 2010-03-30 71 0.19800 0.781 269493 ... 0.0332 129.998 4 0.326 1 1 Usher OMG (feat. will.i.am) 3 30 1 5 rows × 24 columns Visualisation In [24]: #%%capture % matplotlib inline from matplotlib import pyplot as plt fig = plt . figure ( figsize = ( 16 , 8 )) ax = fig . add_axes ([ 0 , 0 , 1 , 1 ]) pop = [ 'non_Hit' , 'Hit' ] pop_count = list ( df [ 'hit' ] . value_counts ()) ax . bar ( pop , pop_count , color = '#293484' ) for i , v in enumerate ( pop_count ): ax . text ( i -. 05 , v + 100 , pop_count [ i ], fontsize = 12 , color = 'black' ) plt . title ( 'Distribution des nombres de chansons \\n suivant les modalités de la variable hit.' ) plt . ylabel ( 'nombre_de_chansons' ) plt . xlabel ( 'classe' ) plt . savefig ( 'plot.png' , dpi = 200 , bbox_inches = 'tight' ) plt . show () La distribution de la variable hit est telle que seulement 902 chansons du jeu de données sont hits, alors que 18280 ne le sont pas. Il y a lieu de contaster un déséquilibre flagrant au sein de notre variable d'interêt. On règlera ce souci plus tard avant de passer à la phase d'entrainement de notre modèle. In [25]: %%capture fig ,( ax1 , ax2 , ax3 ) = plt . subplots ( 1 , 3 , figsize = ( 18 , 8 )) fig . suptitle ( 'Date de sortie des musiques hits' ) ax1 . set_title ( 'Mois' ) ax1 . hist ( df [ df [ 'hit' ] == 1 ][ 'mois_sortie' ], bins = 30 ) ax2 . set_title ( 'Jour de la semaine' ) ax2 . hist ( df [ df [ 'hit' ] == 1 ][ 'jour_sem_sortie' ], bins = 14 ) ax3 . set_title ( 'Jour' ) ax3 . hist ( df [ df [ 'hit' ] == 1 ][ 'jour_sortie' ], bins = 62 ) plt . savefig ( r 'C:\\Users\\Kathee\\Documents\\Blog content\\img\\plot1.png' , dpi = 200 , bbox_inches = 'tight' ) plt . show () A la lumière du graphique ci-dessus, janvier est le mois durant lequel plus de musiques hits sont sorties; juillet étant le mois le moins solicité. On remarque aussi que la majorité des hits de la période sont sortis le 4ème jour de la semaine; soit vendredi. Enfin le premier jour du mois est largement plus utilisé pour publier une chanson. Publier une chanson le premier janvier, c'est probablement un pas vers l'optimisation des chances de sa viralité. Quels sont les artistes les plus populaires sur la période considérée? In [26]: #%%capture plt . figure ( figsize = ( 16 , 8 )) artistes = df [ df [ 'hit' ] == 1 ][ 'nom_artiste' ] . value_counts ()[: 20 ] artistes . plot ( kind = \"bar\" ) plt . title ( \"Les artistes avec le plus de musiques hits\" ) plt . savefig ( r 'C:\\Users\\Kathee\\Documents\\Blog content\\img\\plot2.png' , dpi = 200 , bbox_inches = 'tight' ) plt . show () Sans surprise aucune, les artistes comme Drake, Rihanna et Taylor Swift ont enregistré plus de musiques hits que quiconque autre artiste sur la période. Ceci étant dit, il n'est pas insensé de croire qu'une chanson a plus de chances d'être virale si elle contient l'un de ces artistes. In [27]: #%%capture plt . figure ( figsize = ( 16 , 8 )) plt . hist ( df [ df [ 'hit' ] == 1 ][ 'pop_artiste' ], bins = 100 , density = True , alpha = 0.5 , label = \"Hit\" ) plt . hist ( df [ df [ 'hit' ] == 0 ][ 'pop_artiste' ], bins = 100 , density = True , alpha = 0.5 , label = \"Not Hit\" ) plt . title ( \"Popularité des artistes\" ) plt . legend () plt . savefig ( r 'C:\\Users\\Kathee\\Documents\\Blog content\\img\\plot3.png' , dpi = 200 , bbox_inches = 'tight' ) plt . show () Les deux distributions sont assymétriques à droite. Cependant, l'assymétrie de la distribution des musiques hits est plus prononcée. 75 % des musiques non hits ont des artistes dont la popularité est inférieure à 82, alors que seulement la popularité des artistes de 50 % des musiques hits est inférieure à ce nombre. pop_artiste pourrait être une variable importante de notre modèle. Quid la répartitition des variables d'audio au sein des deux catégories? In [28]: #%%capture import numpy as np from matplotlib import pyplot as plt features_hit = df . loc [ df [ 'hit' ] == 1 ,[ 'acoustique' , 'dansabilite' , 'energie' , 'instrumentalite' , 'vivacite' , 'eloquence' , 'valence' ]] features_non_hit = df . loc [ df [ 'hit' ] == 0 ,[ 'acoustique' , 'dansabilite' , 'energie' , 'instrumentalite' , 'vivacite' , 'eloquence' , 'valence' ]] N = len ( features_hit . mean ()) #Liste des nombre de variables ind = np . arange ( N ) width = 0.25 #diagrame en batons avec la liste des musiques hits plt . barh ( ind , features_hit . mean () , width , label = 'chansons hits' , color = 'lightslategray' ) #diagrame en batons avec la liste des musiques non hits plt . barh ( ind + width , features_non_hit . mean (), width , label = 'chansons non-hits' , color = 'mediumvioletred' ) #X- label plt . xlabel ( 'Moyenne' , fontsize = 12 ) # Title plt . title ( \"Distribution des valeurs moyennes \\n des caractéristiques d'audio des chansons.\" ) #Vertical ticks plt . yticks ( ind + width / 2 , ( list ( features_non_hit )[:]), fontsize = 12 ) #legend plt . legend ( loc = 'best' ) # Figure size plt . rcParams [ 'figure.figsize' ] = ( 16 , 8 ) # Set style plt . style . use ( \"ggplot\" ) plt . savefig ( r 'C:\\Users\\Kathee\\Documents\\Blog content\\img\\plot4.png' , dpi = 200 , bbox_inches = 'tight' ) plt . show () Les variables prédominantes au sein des deux catégories sont les mêmes : energie , dansabilité et valence . La seule difference est que les valeurs moyennées de ces variables sont plus élevées pour la catégorie des chansons hits. Ceci atteste que les musiques hits sont plus rapides et sonores. Elles sont plus adaptées à la danse et sont plus enclines à inspirer la joie, la gaité, l'euphorie. L'approche de machine learning Jusque-là, nous avons pu découvrir quelques interéssants insights sur les données. Afin d'écourter cet article, passons directement en la partie concernant l'algorithme de machine learning que nous allons utiliser. Nous allons construire un modèle afin de prédire la classe, hit ou non hit, qu'une musique est la plus suceptible d'appartenir en nous basant sur un ensemble de variables explicatives, comme expliqué au début. Il est important de garder en tête que nous voulons un modèle qui soit le plus performant possible. En outre, nous allons utiliser l'algorithme de classification de LightGBM . Pourquoi? A cause du fait que j'ai découvert ce framework recemment alors que je travaillais sur une compétition de Kaggle. J'ai vu tout le bien qu'on en disait. Depuis, je me suis mis à l'étudier et à l'implementer. En guise d'expliquer comment l'algorithme fonctionne, on se contentera de mentionner que c'est une approche de machine learning supervisé que l'on va utiliser et que LightGBM est un framework de gradient boosting qui utilise des algorithmes d'apprentissage basés sur des arbres. Il est conçu pour être distribué et efficient avec les avantages suivants: Vitesse d'entraînement plus rapide et efficacité plus élevée. Utilisation réduite de la mémoire. Meilleure précision. Prise en charge de l'apprentissage parallèle et GPU. Capable de gérer des données à grande échelle. Suppression de variables La première étape dans la préparation de notre jeu de données pour l'entrainement du modèle est de nous assurer que toutes les colonnes sont de valeurs numériques. C'est en ce sens que nous avons supprimé les variables comme nom_chanson, search, nom_artiste, date_sortie, pop_chanson, jour_sortie, jour_sem_sortie et mois_sortie. In [29]: df . drop ([ 'nom_chanson' , 'search' , 'nom_artiste' , 'date_sortie' , 'jour_sem_sortie' , 'jour_sortie' , 'mois_sortie' , 'time_signature' ], axis = 1 , inplace = True ) Feature engeneering A ce stade on recourt à plusieurs techniques afin d'optimiser notre jeu de données. En realité, on en a déjà appliqué quelques-unes au début lorsqu'il était question de gérer les valeurs manquantes, les dupplications, l'extraction des dates, One-Hot-Encoding, etc. A présent nous allons appliquer la technique dite scaling sur quelques variables du jeu. Cette technique a la vertu de mettre toutes les variables numériques sur une même base comparable. Bien, à quoi ressemble maintenant notre jeu? In [30]: from sklearn.preprocessing import MinMaxScaler scaler = MinMaxScaler () df [[ 'acoustique' , 'dansabilite' , 'energie' , 'instrumentalite' , 'vivacite' , 'intensite' , 'eloquence' , 'valence' , 'tempo' , 'pop_artiste' , 'tot_followers' , 'marche_disp' , 'duree' , 'pop_chanson' ]] = scaler . fit_transform ( df [[ 'acoustique' , 'dansabilite' , 'energie' , 'instrumentalite' , 'vivacite' , 'intensite' , 'eloquence' , 'valence' , 'tempo' , 'pop_artiste' , 'tot_followers' , 'marche_disp' , 'duree' , 'pop_chanson' ]]) In [31]: df . head ( n = 2 ) Out[31]: pop_artiste tot_followers marche_disp pop_chanson acoustique dansabilite duree energie instrumentalite vivacite intensite eloquence tempo valence hit featuring 0 0.81 0.084919 1.000000 0.822917 0.099498 0.771195 0.031525 0.836997 0.000000 0.291919 0.933029 0.149474 0.545336 0.721212 1 0 1 0.74 0.045538 0.987179 0.718750 0.093072 0.599591 0.046020 0.621992 0.000636 0.202020 0.882599 0.031895 0.490429 0.233333 1 0 Vueillez remarquer que toutes les variables quantitatives indépendantes sont comprises dans l'intervalle [0,1]. Dans la réalité, cela ne fait aucun sens qu'une variable tel que tot_followers par exemple contienne des valeurs entre 0 et 1. Mais du point de vue de machine learning, cela a tout son sens car c'est par ce procédé que les variables deviennent comparables. Les algorithmes de ML fonctionnent mieux lorsque les variables indépendantes sont relativement à une échelle similaire et proches de la distribution normale. Suréchantillonage des données Rappelons-nous qu'au regard de la variable dépendante, notre jeu de données est très déséquilibré. La catégorie hit est sous-représentée par rapport à la classe non_hit. Pour y remédier, nous allons recourir à la téchinque SMOTE. SMOTE (Synthetic Minority Over-Sampling TEchnique) est une technique utilisée pour traiter des ensembles de données déséquilibrées. Introduite pour la première fois par Nitesh V. Chawla, SMOTE est une technique basée sur les plus proches voisins avec une distance Euclidienne entre les points de données dans l'espace des caractéristiques. In [32]: from imblearn.over_sampling import SMOTE sm = SMOTE ( random_state = 42 ) X = df [[ 'acoustique' , 'dansabilite' , 'energie' , 'instrumentalite' , 'vivacite' , 'intensite' , 'eloquence' , 'valence' , 'tempo' , 'duree' ]] y = df [ 'hit' ] X_res , y_res = sm . fit_resample ( X , y ) # Visualisation du changement y_res . value_counts () Out[32]: 1 18280 0 18280 Name: hit, dtype: int64 On remarque que les deux classes ont bien le même nombre d'observations. L'entrainement du modèle se fera donc sur des données équilibrées. Le critère de performance dont on se servira pour évaluer le modèle est bien accuracy . Modèle Tout est fin prêt pour entraîner et évaluer notre fameux modèle. In [33]: import lightgbm as lgb from sklearn.model_selection import train_test_split from sklearn.metrics import auc , accuracy_score , roc_auc_score , roc_curve , confusion_matrix % matplotlib inline import seaborn as sns import matplotlib.pyplot as plt # Fractionnement du dataset en deux sous-jeux: un pour l'entrainement et l'autre pour tester X_train , X_test , y_train , y_test = train_test_split ( X_res , y_res , test_size = 0.30 , random_state = 20 ) # Paramètres optimisés clf = lgb . LGBMClassifier ( boosting_type = 'gbdt' , class_weight = None , colsample_bytree = 1.0 , importance_type = 'split' , learning_rate = 0.1 , max_depth =- 1 , min_child_samples = 20 , min_child_weight = 0.001 , min_split_gain = 0.0 , n_estimators = 100 , n_jobs =- 1 , num_leaves = 31 , objective = None , random_state = None , reg_alpha = 0.0 , reg_lambda = 0.0 , silent = True , subsample = 1.0 , subsample_for_bin = 200000 , subsample_freq = 0 ) clf . fit ( X_train , y_train ) # évaluation de la performance du modèle y_pred = clf . predict ( X_test ) accuracy = accuracy_score ( y_pred , y_test ) print ( 'Score de précision du modèle de LightGBM : {0:0.4f} ' . format ( accuracy )) Score de précision du modèle de LightGBM : 0.8658 La précision du modèle atteint les 86 %, ce qui est très bien. La visualisation de la matrice de confusion nous permet de comprendre les erreurs commise par notre modèle-classeur. In [34]: cm = confusion_matrix ( y_test , y_pred ) mc_matrice = pd . DataFrame ( data = cm , columns = [ 'Classe réelle:0' , 'Classe réelle:1' ], index = [ 'Classe prédite:0' , 'Classe prédite:1' ]) plt . figure ( figsize = ( 16 , 8 )) ax1 = plt . axes () sns . heatmap ( mc_matrice , ax = ax1 , annot = True , fmt = 'd' , cmap = 'YlGnBu' ) ax1 . set_title ( 'Matrice de confusion' ) plt . show () La matrice de confusion est une matrice qui mesure la qualité d'un système de classification. Le diagonal principal représente les observations clasées correctement par le modèle, et le diagonal secondaire, celles classées incorrectement. De ce fait, l'erreur la plus fréquente commise par le modèle est d'avoir classé une chanson en tant que non-hit alors qu'en realité elle était hit (1039 cas), l'erreur de type II plus précisément. Nous pouvons comprendre à présent les variables explicatives ayant le plus de poids dans le processus de classification du modèle en visualisant le graphique de l'importance. In [35]: #plt.figure(figsize=(16,8)) ax = lgb . plot_importance ( clf , height = 0.4 , max_num_features = 18 , importance_type = 'split' , xlim = ( 0 , 500 ), ylim = ( 0 , 10 ), figsize = ( 16 , 8 )) plt . show () Tempo est la variable explicative la plus importante du modèle pour la prédiction de la variable hit . Ensuite, les variables explicatives les plus pertinentes sont l' acoustique et la valence , et enfin viennent les autres variables relatives à l'audio. Plus de données, meilleur modèle Après ajout de variables relatives à l'artiste telles que pop_artiste , tot_followers , marche_disp et pop_chanson , la précision du modèle atteint les 98 %, ce qui est excellent comme niveau de performance. Notre modèle peut prédire si une musique va etre hit ou non, seulement en se basant sur les variables relatives a la chanson de maniere generale, avec une probabilité superieure a 0.98. In [36]: X = df [[ 'acoustique' , 'dansabilite' , 'energie' , 'instrumentalite' , 'vivacite' , 'intensite' , 'eloquence' , 'valence' , 'tempo' , 'duree' , 'featuring' , 'pop_artiste' , 'tot_followers' , 'marche_disp' , 'pop_chanson' ]] y = df [ 'hit' ] In [37]: X_res , y_res = sm . fit_resample ( X , y ) X_train , X_test , y_train , y_test = train_test_split ( X_res , y_res , test_size = 0.30 , random_state = 20 ) clf . fit ( X_train , y_train ) y_pred = clf . predict ( X_test ) accuracy = accuracy_score ( y_pred , y_test ) print ( 'LightGBM Model accuracy score: {0:0.4f} ' . format ( accuracy )) y_pred_train = clf . predict ( X_train ) print ( 'Training-set accuracy score: {0:0.4f} ' . format ( accuracy_score ( y_train , y_pred_train ))) LightGBM Model accuracy score: 0.9749 Training-set accuracy score: 0.9859 In [38]: ax = lgb . plot_importance ( clf , height = 0.4 , max_num_features = 18 , importance_type = 'split' , xlim = ( 0 , 800 ), ylim = ( 0 , 16 ), figsize = ( 16 , 8 )) plt . show () In [39]: cm = confusion_matrix ( y_test , y_pred ) mc_matrice = pd . DataFrame ( data = cm , columns = [ 'Classe réelle:0' , 'Classe réelle:1' ], index = [ 'Classe prédite:0' , 'Classe prédite:1' ]) plt . figure ( figsize = ( 16 , 8 )) ax2 = plt . axes () sns . heatmap ( mc_matrice , ax = ax2 , annot = True , fmt = 'd' , cmap = 'YlGnBu' ) ax2 . set_title ( 'Matrice de confusion' ) plt . show () Conclusion et perspectives In [ ]:","tags":"About","url":"https://JBobyM.github.io/predire-musique-hit-en-utilisant-spotify.html","loc":"https://JBobyM.github.io/predire-musique-hit-en-utilisant-spotify.html"},{"title":"Juste pour tester","text":"Ce poste est a supprimer! J'ai ecrit ce poste pour la seule raison d'experimenter comment enlever un article sur mon blog. C'est justement parcequ'il ne renferme aucune valeur ajoutee que je puisse me permettre d'ecrire les lignes qui suivent. Lionel Messi, parfois surnommé Leo Messi, né le 24 juin 1987 à Rosario, est un footballeur international argentin évoluant au poste d'attaquant au FC Barcelone. Seul footballeur sextuple Ballon d'Or et Soulier d'or européen, Messi est considéré comme l'un des meilleurs footballeurs de l'histoire. Joueur créatif complet, il est un buteur prolifique détenteur de nombreux records : sextuple vainqueur du Pichichi, il est le meilleur buteur de l'histoire du championnat d'Espagne, du FC Barcelone, de la sélection argentine, et le deuxième meilleur buteur de l'histoire de la Ligue des champions. Auteur de plus de 700 buts en carrière, il est actuellement le septième meilleur buteur de l'histoire du football en matchs officiels4,5. Il détient depuis 2012 le record mondial du nombre de buts inscrits sur une saison (73), sur une année civile (91), et le record de buts sur une saison dans un championnat européen (50). Messi se distingue également comme étant le meilleur dribbleur du monde6 et l'un des meilleurs passeurs décisifs de ce sport, étant le meilleur passeur de l'histoire du championnat d'Espagne7, de la Copa América, et le deuxième de la Ligue des champions derrière Cristiano Ronaldo. En février 2020, il devient le premier footballeur à recevoir le prestigieux Prix Laureus du meilleur sportif de l'année8. Débutant le football dans sa ville natale de Rosario au sein du club des Newell's Old Boys, il est atteint d'un problème de croissance lorsqu'il rejoint à treize ans le centre de formation du FC Barcelone, qui financera son traitement hormonal. Joueur d'un seul club depuis ses débuts professionnels à seize ans, il a depuis disputé plus de 680 matches sous les couleurs blaugrana, devenant ainsi le deuxième joueur le plus capé de l'histoire du club catalan. Avec trente-six titres remportés en carrière dont trente-quatre avec son club9, il est le joueur barcelonais et argentin le plus titré de l'histoire et possède l'un des plus beaux palmarès de son sport. Avec Barcelone, il a notamment remporté quatre Ligues des champions, dix championnats d'Espagne et six Coupes d'Espagne. L'année 2009 est marqué par la réalisation d'un triplé (Ligue des Champions, Coupe, Championnat) et même d'un sextuplé inédit historique10. Un triplé qui sera réédité en 201511 pour la première fois par un même club12. Avec son pays, il a remporté la Coupe du monde des moins de 20 ans en 2005 ainsi qu'une médaille d'or aux Jeux olympiques de 2008. In [ ]:","tags":"articles","url":"https://JBobyM.github.io/test-no-value.html","loc":"https://JBobyM.github.io/test-no-value.html"},{"title":"Test test","text":"Test 1 2 Test 1 2, 1 2 In [ ]:","tags":"About","url":"https://JBobyM.github.io/test-test.html","loc":"https://JBobyM.github.io/test-test.html"},{"title":"Couple ways to stay productive with personal development","text":"Oula oula the most sane and apt decisions you can make for yourself is to strive towards continuous self-development. Some of the things that we all want for ourselves include: enhancing the quality of our lives, achieving more, becoming better people, and trying to be a better version of ourselves. That's why we set personal development goals in our lives. Listed below are 21 personal development goals example that will aid and augment your personal growth journey into a happier more confident you SOLUTIONS S.A . Embrace Empathy. Empathy is about objectively comprehending differing perspective which in turn provides a wealth of insight into your own perspective. Confidence. Studies have revealed that an individual's IQ is not the most important component for success. Instead, the following three factors are considered to be much more important than intelligence in determining success: self-confidence, goal setting, and perseverance. And individual's self esteem, or your self-confidence, is basically what he or she thinks about himself or herself. One of the reasons for boosting your self-confidence is that there is a strong association between confidence and success. Hence, one of your self-development goals should definitely to to boost your self-confidence. Listen Actively. Actively learn to pay attention and demonstrate to others that you truly value their opinions and what they have to say. Choose active listening, open-ended question, with supporting body language, and remove any distractions that impede with your ability to listen. Make fear your friend. To eliminate fear, you have to first be exposed to fear allow yourself to feel afraid and expose yourself to it. Once you are comfortable with the ambiguity and uncertainly of the situation, you can start working your way through it in a calm rational manner. Improve your body language. Your body language is nothing but non-verbal communication which includes the gestures and movements you project. Research has proved that the correct body language can help you connect effectively with others and convey your message across more efficiently. It conveys your assertiveness, confidence, and perseverance. In fact, certain body postures can also help to Improve your performance. Get along with others. You must always look for means to create rapport with others. However, you need to be honest and your primary objective should not be to manipulate others, rather your should learn the ways through which you can relate and get along well with others. Get along with yourself. Getting along with yourself is a precursor to getting along with others. You must learn to appreciate and accept your skills, experience, philosophies, aspirations, and limitations. This healthy focus and more grounded you, is inspirational and charismatic. Stop procrastinating. You need to understand that procrastination is not a character trait, but rather a habit. Since it is a habit, it is 100% possible for you to unlearn it just as you have learnt the habit. Strive to utilize time is the most effective manner and avoid procrastination by all means. Wake up Early. Develop the habit to get up early. The age old proverb which says: \"Early to bed and early to rise makes a man healthy, wealthy, and wise!\" has been coined owing to the multiple benefits of an early riser. Some of these include: watching and enjoying the sun rise, do some early morning exercise for your fitness, being able to work on a project just because it's important to you before the day officially gets started, and so on. In addition, studies show that early rises are happier, healthier, and more productive than their late rising counterparts. Become more proactive. Inculcate the habit of proactiveness. Some of the traits of proactive people are : They consider themselves to be the creators of their lives. They don't consider themselves to be victims of external conditions. They don't allow other to determine their fate. The take ownership of the responsibility for the results the they get. Master the art of conflict resolution. Conflict is a part and parcel of life. The key is to develop the skill of conflict resolution. If you posses the ability to resolve conflicts rationally and settle disputes amicably, it will certainly make more successful and happy. Let go of the past. One of the biggest hindrances to personal growth is holding on to the past. In order to be happy in the true sense of the term, it is very important to be in the present. So, you must learn to release the ghosts of the past and clear skeletons from the cupboard. Read more. Read often. Nothing is more powerful than the treasure of knowledge and the way to acquire this treasure is to read as much as you can. Research has proved that acquiring new knowledge satisfies an individual's thirst for competence, which makes them eventually happier. You must develop the habit of reading books that will help you to acquire new skills and as well as to polish your existing skills. Become more resilient. Resilience is the innate ability to overcome and kind of adversity . It is the difference between feeling helpless and facing your problems confidently and bravely. You must always learn how to bounce back from any kind of problem; it will only help you to emerge as a stronger individuals. Manage stress effectively. Too much stress can land you up in distress. It impacts not your physical health but also your mental and emotional health. You must know how to effectively manage stress. There are a lot of stress management techniques available these days. So all you need to develop is the willingness to fight stress. Finding the means to tackle stress is not a challenge these days with help available at the click of a mouse. Ignore your limitations. Limiting beliefs hinders your progress by keeping you caught up in your comfort zone and prevents you from trying out new things and stops you from tacking risks for the fear of failure or getting hurt in the process. You can have limiting beliefs about almost anything ranging from money, relationship, success, and the list is endless. It is important to identify your limiting beliefs, conquer them, and reinstate them with positive beliefs that enable you to achieve your dreams in life. Share yourself. Become a teacher and share your time, feedback, opinions knowledge and skills. The transfer of knowledge and skills not only benefits the other person but the process it self reinforces the neural pathways of what you know taking you closer to mastery. Increase your willpower. Harbouring a strong will power enhances your finances, your health, your relationship, your professional success, and all other areas of your life. Irrespective of the goals that you have set for yourself, you need willpower in order to attain them. Become more mindful. Strive to become more mindful. It helps you to acknowledge the abundance and the benefits that is already a part of your lives. Instead of spending time thinking about the past which you have no idea of how it is going to be, you must learn to live in the present and enjoy the moments. Make better decisions. Develop the habit to make better decisions in life. The choices or the decisions that you make determines the life that you will eventually lead. Your decisions share your life, so it;s all the more important that the better decisions you make, the better your life is likely to be. Work on your growth mindset. Your attitude defines who you are as a person and it the identification point of the state of your mind. You must constantly work on your attitude and strive to make it one conducive for growth. A positive mental attitude can move mountains for you. It helps you to achieve what your want from life. The above mentioned personal development goals have been listed so that you are able to life of your dreams by setting achievable personal goals that make you a better person. So, it's never too late. In [ ]: In [ ]:","tags":"About","url":"https://JBobyM.github.io/couple-ways-to-stay-productive-with-personal-development.html","loc":"https://JBobyM.github.io/couple-ways-to-stay-productive-with-personal-development.html"},{"title":"First Jupyter Post","text":"First post! This will be the first post in our new Pelican blog! You can add any code or markdown cells to this Jupyter notebook, and they will be rendered on your blog. In [1]: import random random . randint ( 0 , 100 ) Out[1]: 50 In [2]: import matplotlib.pyplot as plt % matplotlib inline plt . hist ([ 10 , 5 , 7 , 10 , 1 , 1 , 2 , 3 , 5 ]) Out[2]: (array([2., 1., 1., 0., 2., 0., 1., 0., 0., 2.]), array([ 1. , 1.9, 2.8, 3.7, 4.6, 5.5, 6.4, 7.3, 8.2, 9.1, 10. ]), <a list of 10 Patch objects>) In [ ]:","tags":"articles","url":"https://JBobyM.github.io/first-jup-post.html","loc":"https://JBobyM.github.io/first-jup-post.html"},{"title":"First Post on My Sweet New Blog","text":"I am On My Way To Internet Fame and Fortune! This is my first post on my new blog. While not super informative it should convey my sense of excitement and eagerness to engage with you, the reader! Tables Option Description data path to data files to supply the data that will be passed into templates. engine engine to be used for processing templates. Handlebars is the default. ext extension to be used for dest files. Right aligned columns Option Description data path to data files to supply the data that will be passed into templates. engine engine to be used for processing templates. Handlebars is the default. ext extension to be used for dest files. Links link text link with title Autoconverted link https://github.com/nodeca/pica (enable linkify to see) Images Like links, Images also have a footnote style syntax With a reference later in the document defining the URL location: In [ ]:","tags":"articles","url":"https://JBobyM.github.io/first-post-on-my-sweet-new-blog.html","loc":"https://JBobyM.github.io/first-post-on-my-sweet-new-blog.html"}]};